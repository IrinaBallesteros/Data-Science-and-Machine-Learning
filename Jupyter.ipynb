{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.10.6' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def OutputRocketText():\n",
    "  print(5)\n",
    "  print(4)\n",
    "  print(3)\n",
    "  print(2)\n",
    "  print(1)\n",
    "  print(\"Rocket will be launching soon!\")\n",
    "  \n",
    "  return\n",
    "\n",
    "OutputRocketText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "strPath = \"text.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObject = open(strPath, \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rocket will launch soon!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rocketNumber' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m      7\u001b[0m OutputRocketText()\n\u001b[0;32m----> 8\u001b[0m \u001b[39mprint\u001b[39m(rocketNumber)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rocketNumber' is not defined"
     ]
    }
   ],
   "source": [
    "def OutputRocketText():\n",
    "    rocketNumber = 1\n",
    "    print(\"Rocket will launch soon!\")\n",
    "\n",
    "    return\n",
    "\n",
    "OutputRocketText()\n",
    "print(rocketNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'rocketText' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m     rocketText \u001b[39m=\u001b[39m rocketText \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m two days\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m OutputRocketText()\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mOutputRocketText\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mOutputRocketText\u001b[39m():\n\u001b[0;32m----> 3\u001b[0m     rocketText \u001b[39m=\u001b[39m rocketText \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m two days\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'rocketText' referenced before assignment"
     ]
    }
   ],
   "source": [
    "rocketText = \"We will launch in\"\n",
    "def OutputRocketText():\n",
    "    rocketText = rocketText + \" two days\"\n",
    "    return\n",
    "\n",
    "OutputRocketText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will launch in two days\n"
     ]
    }
   ],
   "source": [
    "rocketText = \"We will launch in\"\n",
    "\n",
    "def OutputRocketText():\n",
    "    global rocketText\n",
    "    rocketText = rocketText + \" two days\"\n",
    "    return\n",
    "\n",
    "OutputRocketText()\n",
    "print(rocketText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will launch in two days\n"
     ]
    }
   ],
   "source": [
    "def OutputRocketText(textInput):\n",
    "    textInput = textInput + \" two days\"\n",
    "    return textInput\n",
    "\n",
    "rocketText = \"We will launch in\"\n",
    "newRocketText = OutputRocketText(rocketText)\n",
    "print(newRocketText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 50, 47, 97, 49, 3, 53, 42, 26, 74, 82, 62, 37, 15, 70, 27, 36, 35, 48, 52, 63, 64]\n"
     ]
    }
   ],
   "source": [
    "data = [50,50,47,97,49,3,53,42,26,74,82,62,37,15,70,27,36,35,48,52,63,64]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m grades \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(data)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(grades)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "grades = np.array(data)\n",
    "print(grades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (type(data),'x 2:', data * 2)\n",
    "print('---')\n",
    "print (type(grades),'x 2:', grades * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an array of study hours\n",
    "study_hours = [10.0,11.5,9.0,16.0,9.25,1.0,11.5,9.0,8.5,14.5,15.5,\n",
    "               13.75,9.0,8.0,15.5,8.0,9.0,6.0,10.0,12.0,12.5,12.0]\n",
    "\n",
    "# Create a 2D array (an array of arrays)\n",
    "student_data = np.array([study_hours, grades])\n",
    "\n",
    "# display the array\n",
    "student_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean value of each sub-array\n",
    "avg_study = student_data[0].mean()\n",
    "avg_grade = student_data[1].mean()\n",
    "\n",
    "print('Average study hours: {:.2f}\\nAverage grade: {:.2f}'.format(avg_study, avg_grade))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m df_students \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mDan\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJoann\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPedro\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRosie\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEthan\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mVicky\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFrederic\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJimmie\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                                      \u001b[39m'\u001b[39m\u001b[39mRhonda\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGiovanni\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFrancesca\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mRajab\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mNaiyana\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mKian\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJenny\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                      \u001b[39m'\u001b[39m\u001b[39mJakeem\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mHelena\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mIsmat\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mAnila\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mSkye\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mDaniel\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mAisha\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m                             \u001b[39m'\u001b[39m\u001b[39mStudyHours\u001b[39m\u001b[39m'\u001b[39m:student_data[\u001b[39m0\u001b[39m],\n\u001b[1;32m      7\u001b[0m                             \u001b[39m'\u001b[39m\u001b[39mGrade\u001b[39m\u001b[39m'\u001b[39m:student_data[\u001b[39m1\u001b[39m]})\n\u001b[1;32m      9\u001b[0m df_students \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_students = pd.DataFrame({'Name': ['Dan', 'Joann', 'Pedro', 'Rosie', 'Ethan', 'Vicky', 'Frederic', 'Jimmie', \n",
    "                                     'Rhonda', 'Giovanni', 'Francesca', 'Rajab', 'Naiyana', 'Kian', 'Jenny',\n",
    "                                     'Jakeem','Helena','Ismat','Anila','Skye','Daniel','Aisha'],\n",
    "                            'StudyHours':student_data[0],\n",
    "                            'Grade':student_data[1]})\n",
    "\n",
    "df_students "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Flights Data Exploration Challenge\n",
    "\n",
    "In this challenge, you'll explore a real-world dataset containing flights data from the US Department of Transportation.\n",
    "\n",
    "Let's start by loading and viewing the data.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_flights = pd.read_csv('data/flights.csv')\n",
    "df_flights.head()\n",
    "\n",
    "The dataset contains observations of US domestic flights in 2013, and consists of the following fields:\n",
    "\n",
    "    Year: The year of the flight (all records are from 2013)\n",
    "    Month: The month of the flight\n",
    "    DayofMonth: The day of the month on which the flight departed\n",
    "    DayOfWeek: The day of the week on which the flight departed - from 1 (Monday) to 7 (Sunday)\n",
    "    Carrier: The two-letter abbreviation for the airline.\n",
    "    OriginAirportID: A unique numeric identifier for the departure aiport\n",
    "    OriginAirportName: The full name of the departure airport\n",
    "    OriginCity: The departure airport city\n",
    "    OriginState: The departure airport state\n",
    "    DestAirportID: A unique numeric identifier for the destination aiport\n",
    "    DestAirportName: The full name of the destination airport\n",
    "    DestCity: The destination airport city\n",
    "    DestState: The destination airport state\n",
    "    CRSDepTime: The scheduled departure time\n",
    "    DepDelay: The number of minutes departure was delayed (flight that left ahead of schedule have a negative value)\n",
    "    DelDelay15: A binary indicator that departure was delayed by more than 15 minutes (and therefore considered \"late\")\n",
    "    CRSArrTime: The scheduled arrival time\n",
    "    ArrDelay: The number of minutes arrival was delayed (flight that arrived ahead of schedule have a negative value)\n",
    "    ArrDelay15: A binary indicator that arrival was delayed by more than 15 minutes (and therefore considered \"late\")\n",
    "    Cancelled: A binary indicator that the flight was cancelled\n",
    "\n",
    "Your challenge is to explore the flight data to analyze possible factors that affect delays in departure or arrival of a flight.\n",
    "\n",
    "    Start by cleaning the data.\n",
    "        Identify any null or missing data, and impute appropriate replacement values.\n",
    "        Identify and eliminate any outliers in the DepDelay and ArrDelay columns.\n",
    "    Explore the cleaned data.\n",
    "        View summary statistics for the numeric fields in the dataset.\n",
    "        Determine the distribution of the DepDelay and ArrDelay columns.\n",
    "        Use statistics, aggregate functions, and visualizations to answer the following questions:\n",
    "            What are the average (mean) departure and arrival delays?\n",
    "            How do the carriers compare in terms of arrival delay performance?\n",
    "            Is there a noticable difference in arrival delays for different days of the week?\n",
    "            Which departure airport has the highest average departure delay?\n",
    "            Do late* departures tend to result in longer arrival delays than on-time departures?*\n",
    "            Which route (from origin airport to destination airport) has the most late* arrivals?*\n",
    "            Which route has the highest average arrival delay?\n",
    "\n",
    "Add markdown and code cells as required to create your solution.\n",
    "\n",
    "    Note: There is no single \"correct\" solution. A sample solution is provided in 01 - Flights Challenge.ipynb.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "!pip install statsmodels\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/doggy-boot-harness.csv\n",
    "\n",
    "# Load a file containing dog's boot and harness sizes\n",
    "data = pandas.read_csv('doggy-boot-harness.csv')\n",
    "\n",
    "# Print the first few rows\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Fit a simple model that finds a linear relationship\n",
    "# between boot size and harness size, which we can use later\n",
    "# to predict a dog's boot size, given their harness size\n",
    "model = smf.ols(formula = \"boot_size ~ harness_size\", data = data).fit()\n",
    "\n",
    "print(\"Model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model_filename = './avalanche_dog_boot_model.pkl'\n",
    "joblib.dump(model, model_filename)\n",
    "\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = joblib.load(model_filename)\n",
    "\n",
    "print(\"We have loaded a model with the following parameters:\")\n",
    "print(model_loaded.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's write a function that loads and uses our model\n",
    "def load_model_and_predict(harness_size):\n",
    "    '''\n",
    "    This function loads a pretrained model. It uses the model\n",
    "    with the customer's dog's harness size to predict the size of\n",
    "    boots that will fit that dog.\n",
    "\n",
    "    harness_size: The dog harness size, in cm \n",
    "    '''\n",
    "\n",
    "    # Load the model from file and print basic information about it\n",
    "    loaded_model = joblib.load(model_filename)\n",
    "\n",
    "    print(\"We've loaded a model with the following parameters:\")\n",
    "    print(loaded_model.params)\n",
    "\n",
    "    # Prepare data for the model\n",
    "    inputs = {\"harness_size\":[harness_size]} \n",
    "\n",
    "    # Use the model to make a prediction\n",
    "    predicted_boot_size = loaded_model.predict(inputs)[0]\n",
    "\n",
    "    return predicted_boot_size\n",
    "\n",
    "# Practice using our model\n",
    "predicted_boot_size = load_model_and_predict(45)\n",
    "\n",
    "print(\"Predicted dog boot size:\", predicted_boot_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_size_of_boots(selected_harness_size, selected_boot_size):\n",
    "    '''\n",
    "    Calculates whether the customer has chosen a pair of doggy boots that \n",
    "    are a sensible size. This works by estimating the dog's actual boot \n",
    "    size from their harness size.\n",
    "\n",
    "    This returns a message for the customer that should be shown before\n",
    "    they complete their payment \n",
    "\n",
    "    selected_harness_size: The size of the harness the customer wants to buy\n",
    "    selected_boot_size: The size of the doggy boots the customer wants to buy\n",
    "    '''\n",
    "\n",
    "    # Estimate the customer's dog's boot size\n",
    "    estimated_boot_size = load_model_and_predict(selected_harness_size)\n",
    "\n",
    "    # Round to the nearest whole number because we don't sell partial sizes\n",
    "    estimated_boot_size = int(round(estimated_boot_size))\n",
    "\n",
    "    # Check if the boot size selected is appropriate\n",
    "    if selected_boot_size == estimated_boot_size:\n",
    "        # The selected boots are probably OK\n",
    "        return f\"Great choice! We think these boots will fit your avalanche dog well.\"\n",
    "\n",
    "    if selected_boot_size < estimated_boot_size:\n",
    "        # Selected boots might be too small \n",
    "        return \"The boots you have selected might be TOO SMALL for a dog as \"\\\n",
    "               f\"big as yours. We recommend a doggy boots size of {estimated_boot_size}.\"\n",
    "\n",
    "    if selected_boot_size > estimated_boot_size:\n",
    "        # Selected boots might be too big \n",
    "        return \"The boots you have selected might be TOO BIG for a dog as \"\\\n",
    "               f\"small as yours. We recommend a doggy boots size of {estimated_boot_size}.\"\n",
    "    \n",
    "\n",
    "# Practice using our new warning system\n",
    "check_size_of_boots(selected_harness_size=55, selected_boot_size=39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.box(data,y=[\"month_old_when_trained\", \"standardized_age_when_trained\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost1 = model.cost_history\n",
    "cost2 = model_norm.cost_history\n",
    "\n",
    "# Creates dataframes with the cost history for each model\n",
    "df1 = pandas.DataFrame({\"cost\": cost1, \"Model\":\"No feature scaling\"})\n",
    "df1[\"number of iterations\"] = df1.index + 1\n",
    "df2 = pandas.DataFrame({\"cost\": cost2, \"Model\":\"With feature scaling\"})\n",
    "df2[\"number of iterations\"] = df2.index + 1\n",
    "\n",
    "# Concatenate dataframes into a single one that we can use in our plot\n",
    "df = pandas.concat([df1, df2])\n",
    "\n",
    "# Plot cost history for both models\n",
    "fig = graphing.scatter_2D(df, label_x=\"number of iterations\", label_y=\"cost\", title=\"Training Cost vs Iterations\", label_colour=\"Model\")\n",
    "fig.update_traces(mode='lines')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Obtain the label and feature from the original data\n",
    "dataset = data[['rescues_last_year','weight_last_year']]\n",
    "\n",
    "# Split the dataset in an 70/30 train/test ratio. We also obtain the respective corresponding indices from the original dataset.\n",
    "train, test = train_test_split(dataset, train_size=0.7, random_state=21)\n",
    "\n",
    "print(\"Train\")\n",
    "print(train.head())\n",
    "print(train.shape)\n",
    "\n",
    "print(\"Test\")\n",
    "print(test.head())\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You don't need to understand this code well\n",
    "# It's just used to create a scatter plot\n",
    "\n",
    "# concatenate training and test so they can be graphed\n",
    "plot_set = pandas.concat([train,test])\n",
    "plot_set[\"Dataset\"] = [\"train\"] * len(train) + [\"test\"] * len(test)\n",
    "\n",
    "# Create graph\n",
    "graphing.scatter_2D(plot_set, \"weight_last_year\", \"rescues_last_year\", \"Dataset\", trendline = lambda x: model.params[1] * x + model.params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# First, we define our formula using a special syntax\n",
    "# This says that rescues_last_year is explained by weight_last_year\n",
    "formula = \"rescues_last_year ~ weight_last_year\"\n",
    "\n",
    "# Create and train the model\n",
    "model = smf.ols(formula = formula, data = train).fit()\n",
    "\n",
    "# Graph the result against the data\n",
    "graphing.scatter_2D(train, \"weight_last_year\", \"rescues_last_year\", trendline = lambda x: model.params[1] * x + model.params[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_labels = new_data['rescues_last_year']\n",
    "predicted = model.predict(new_data['weight_last_year'])\n",
    "\n",
    "MSE = mse(correct_labels, predicted)\n",
    "print('MSE = %f ' % MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "!pip install statsmodels\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/dog-training.csv\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/dog-training-switzerland.csv\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "import graphing\n",
    "\n",
    "data = pandas.read_csv(\"dog-training.csv\", delimiter=\"\\t\")\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the label and feature from the original data\n",
    "dataset = data[['rescues_last_year','weight_last_year']]\n",
    "\n",
    "# Print the number of observations\n",
    "print(\"No. observations:\", dataset.shape[0])\n",
    "\n",
    "# Graph the distribution of variables for the unsplit dataset\n",
    "graphing.scatter_2D(dataset, 'rescues_last_year', 'weight_last_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split Dataset using different ratios 50:50, 60:40, 70:30, 80:20\n",
    "train_5050, test_5050 = train_test_split(dataset, test_size=0.5, random_state=2)\n",
    "train_6040, test_6040 = train_test_split(dataset, test_size=0.4, random_state=2)\n",
    "train_7030, test_7030 = train_test_split(dataset, test_size=0.3, random_state=2)\n",
    "train_8020, test_8020 = train_test_split(dataset, test_size=0.2, random_state=2)\n",
    "\n",
    "# Add a column to each set to identify if a datapoint belongs to \"train\" or \"test\"\n",
    "train_5050, test_5050 = train_5050.assign(Set=\"train\"), test_5050.assign(Set=\"test\")\n",
    "train_6040, test_6040 = train_6040.assign(Set=\"train\"), test_6040.assign(Set=\"test\")\n",
    "train_7030, test_7030 = train_7030.assign(Set=\"train\"), test_7030.assign(Set=\"test\")\n",
    "train_8020, test_8020 = train_8020.assign(Set=\"train\"), test_8020.assign(Set=\"test\")\n",
    "\n",
    "# Concatenate the \"train\" and \"test\" sets for each split so we can plot them on the same chart\n",
    "df_5050 = pandas.concat([train_5050, test_5050], axis=0)\n",
    "df_6040 = pandas.concat([train_6040, test_6040], axis=0)\n",
    "df_7030 = pandas.concat([train_7030, test_7030], axis=0)\n",
    "df_8020 = pandas.concat([train_8020, test_8020], axis=0)\n",
    "\n",
    "# Plot each distribution for comparison\n",
    "graphing.scatter_2D(df_5050, \"weight_last_year\", \"rescues_last_year\", title=\"50:50 split\", label_colour=\"Set\", show=True)\n",
    "graphing.scatter_2D(df_6040, \"weight_last_year\", \"rescues_last_year\", title=\"60:40 split\", label_colour=\"Set\", show=True)\n",
    "graphing.scatter_2D(df_7030, \"weight_last_year\", \"rescues_last_year\", title=\"70:30 split\", label_colour=\"Set\", show=True)\n",
    "graphing.scatter_2D(df_8020, \"weight_last_year\", \"rescues_last_year\", title=\"80:20 split\", label_colour=\"Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for each \"train\" set that identifies the split used\n",
    "train_8020 = train_8020.assign(Split=\"80:20\")\n",
    "train_7030 = train_7030.assign(Split=\"70:30\")\n",
    "train_6040 = train_6040.assign(Split=\"60:40\")\n",
    "train_5050 = train_5050.assign(Split=\"50:50\")\n",
    "\n",
    "# Concatenate training sets so we can plot them on the same chart\n",
    "split_df = pandas.concat([train_5050, train_6040, train_7030, train_8020], axis=0)\n",
    "\n",
    " # Plot a histogram of data distribution\n",
    "graphing.multiple_histogram(split_df, label_x=\"rescues_last_year\", label_group=\"Split\", title=\"Distribution of Training data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "def train_and_test_model(name, train, test):\n",
    "    '''\n",
    "    This method creates a model, trains it on the provided data, and assesses \n",
    "    it against the test data\n",
    "    '''\n",
    "    # This formula says that rescues_last_year is explained by weight_last_year\n",
    "    formula = \"rescues_last_year ~ weight_last_year\"\n",
    "\n",
    "    # Create and train a linear regression model using the training data\n",
    "    model = smf.ols(formula = formula, data = train).fit()\n",
    "\n",
    "    # Now evaluate the model (by calculating the Mean Squared Error) using the \n",
    "    # corresponding \"test\" set for that split\n",
    "    correct_answers = test['rescues_last_year']\n",
    "    predictions = model.predict(test['weight_last_year'])\n",
    "    MSE = mse(correct_answers, predictions)\n",
    "    print(name + ' MSE = %f ' % MSE)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train a model and test it for each kind of split\n",
    "print(\"Mean Squared Error values (smaller is better)\")\n",
    "model_5050 = train_and_test_model(\"50:50\", train_5050, test_5050)\n",
    "model_6040 = train_and_test_model(\"60:40\", train_6040, test_6040)\n",
    "model_7030 = train_and_test_model(\"70:30\", train_7030, test_7030)\n",
    "model_8020 = train_and_test_model(\"80:20\", train_8020, test_8020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "# Load some dog statistics from our charity's European arm\n",
    "swiss_data = pandas.read_csv(\"dog-training-switzerland.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Show show information about the data\n",
    "print(f\"The Swiss dataset contains {swiss_data.shape[0]} samples\")\n",
    "graphing.scatter_2D(swiss_data, 'rescues_last_year', 'weight_last_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our models against the swiss data\n",
    "features = swiss_data['weight_last_year']\n",
    "correct_answers = swiss_data['rescues_last_year']\n",
    "\n",
    "# We will now assess our models. Notice we're not training them again.\n",
    "# We are simply testing them against some new data \n",
    "\n",
    "# Assess the model trained on a 50:50 split\n",
    "predictions = model_5050.predict(features)\n",
    "MSE = mse(correct_answers, predictions)\n",
    "print('50:50 MSE = %f ' % MSE)\n",
    "\n",
    "# Assess the model trained on a 60:40 split\n",
    "predictions = model_6040.predict(features)\n",
    "MSE = mse(correct_answers, predictions)\n",
    "print('60:40 MSE = %f ' % MSE)\n",
    "\n",
    "# Assess the model trained on a 70:30 split\n",
    "predictions = model_7030.predict(features)\n",
    "MSE = mse(correct_answers, predictions)\n",
    "print('70:30 MSE = %f ' % MSE)\n",
    "\n",
    "# Assess the model trained on a 80:20 split\n",
    "predictions = model_8020.predict(features)\n",
    "MSE = mse(correct_answers, predictions)\n",
    "print('80:20 MSE = %f ' % MSE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
