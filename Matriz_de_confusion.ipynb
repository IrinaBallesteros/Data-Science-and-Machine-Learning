{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\n",
    "\n",
    "#Import the data from the .csv file\n",
    "dataset = pandas.read_csv('snow_objects.csv', delimiter=\"\\t\")\n",
    "\n",
    "#Let's have a look at the data\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphing # custom graphing code. See our GitHub repo for details\n",
    "\n",
    "# Plot a histogram with counts for each label\n",
    "graphing.multiple_histogram(dataset, label_x=\"label\", label_group=\"label\", title=\"Label distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram with counts for each label\n",
    "graphing.multiple_histogram(dataset, label_x=\"color\", label_group=\"color\", title=\"Color distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphing.box_and_whisker(dataset, label_y=\"size\", title='Boxplot of \"size\" feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphing.box_and_whisker(dataset, label_y=\"roughness\", title='Boxplot of \"roughness\" feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".box_and_whisker(dataset, label_y=\"motion\", title='Boxplot of \"motion\" feature')graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset in an 70/30 train/test ratio. \n",
    "train, test = train_test_split(dataset, test_size=0.3, random_state=2)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False)\n",
    "\n",
    "# Define which features are to be used (leave color out for now)\n",
    "features = [\"size\", \"roughness\", \"motion\"]\n",
    "\n",
    "# Train the model\n",
    "model.fit(train[features], train.label)\n",
    "\n",
    "print(\"Model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a function that measures a models accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate the model's accuracy on the TEST set\n",
    "actual = test.label\n",
    "predictions = model.predict(test[features])\n",
    "\n",
    "# Return accuracy as a fraction\n",
    "acc = accuracy_score(actual, predictions)\n",
    "\n",
    "# Return accuracy as a number of correct predictions\n",
    "acc_norm = accuracy_score(actual, predictions, normalize=False)\n",
    "\n",
    "print(f\"The random forest model's accuracy on the test set is {acc:.4f}.\")\n",
    "print(f\"It correctly predicted {acc_norm} labels in {len(test.label)} predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn has a very convenient utility to build confusion matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Build and print our confusion matrix, using the actual values and predictions \n",
    "# from the test set, calculated in previous cells\n",
    "cm = confusion_matrix(actual, predictions, normalize=None)\n",
    "\n",
    "print(\"Confusion matrix for the test set:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use plotly to create plots and charts\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Create the list of unique labels in the test set, to use in our plot\n",
    "# I.e., ['animal', 'hiker', 'rock', 'tree']\n",
    "x = y = sorted(list(test[\"label\"].unique()))\n",
    "\n",
    "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
    "fig = ff.create_annotated_heatmap(cm, x, y)\n",
    "\n",
    "# Set titles and ordering\n",
    "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
    "                    yaxis = dict(categoryorder = \"category descending\"))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=0.5,\n",
    "                        y=-0.15,\n",
    "                        showarrow=False,\n",
    "                        text=\"Predicted label\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=-0.15,\n",
    "                        y=0.5,\n",
    "                        showarrow=False,\n",
    "                        text=\"Actual label\",\n",
    "                        textangle=-90,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# We need margins so the titles fit\n",
    "fig.update_layout(margin=dict(t=80, r=20, l=100, b=50))\n",
    "fig['data'][0]['showscale'] = True\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Imbalanced data model bias\n",
    "In this exercise, we will take a closer look at imbalanced datasets, what effects they have on predictions and how they can be addressed.\n",
    "\n",
    "We will also employ confusion matrices to evaluate model updates.\n",
    "\n",
    "Data visualization\n",
    "Just like in the previous exercise, we use a dataset that represents different classes of objects found on the mountain:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects_balanced.csv\n",
    "\n",
    "#Import the data from the .csv file\n",
    "dataset = pandas.read_csv('snow_objects.csv', delimiter=\"\\t\")\n",
    "\n",
    "# Let's have a look at the data\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphing # custom graphing code. See our GitHub repo for details\n",
    "\n",
    "# Plot a histogram with counts for each label\n",
    "graphing.multiple_histogram(dataset, label_x=\"label\", label_group=\"label\", title=\"Label distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Add a new label with true/false values to our dataset\n",
    "dataset[\"is_hiker\"] = dataset.label == \"hiker\"\n",
    "\n",
    "# Plot frequency for new label\n",
    "graphing.multiple_histogram(dataset, label_x=\"is_hiker\", label_group=\"is_hiker\", title=\"Distribution for new binary label 'is_hiker'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Custom function that measures accuracy on different models and datasets\n",
    "# We will use this in different parts of the exercise\n",
    "def assess_accuracy(model, dataset, label):\n",
    "    \"\"\"\n",
    "    Asesses model accuracy on different sets\n",
    "    \"\"\" \n",
    "    actual = dataset[label]        \n",
    "    predictions = model.predict(dataset[features])\n",
    "    acc = accuracy_score(actual, predictions)\n",
    "    return acc\n",
    "\n",
    "# Split the dataset in an 70/30 train/test ratio. \n",
    "train, test = train_test_split(dataset, test_size=0.3, random_state=1, shuffle=True)\n",
    "\n",
    "# define a random forest model\n",
    "model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False)\n",
    "\n",
    "# Define which features are to be used (leave color out for now)\n",
    "features = [\"size\", \"roughness\", \"motion\"]\n",
    "\n",
    "# Train the model using the binary label\n",
    "model.fit(train[features], train.is_hiker)\n",
    "\n",
    "print(\"Train accuracy:\", assess_accuracy(model,train, \"is_hiker\"))\n",
    "print(\"Test accuracy:\", assess_accuracy(model,test, \"is_hiker\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn has a very convenient utility to build confusion matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Calculate the model's accuracy on the TEST set\n",
    "actual = test.is_hiker\n",
    "predictions = model.predict(test[features])\n",
    "\n",
    "# Build and print our confusion matrix, using the actual values and predictions \n",
    "# from the test set, calculated in previous cells\n",
    "cm = confusion_matrix(actual, predictions, normalize=None)\n",
    "\n",
    "# Create the list of unique labels in the test set, to use in our plot\n",
    "# I.e., ['True', 'False',]\n",
    "unique_targets = sorted(list(test[\"is_hiker\"].unique()))\n",
    "\n",
    "# Convert values to lower case so the plot code can count the outcomes\n",
    "x = y = [str(s).lower() for s in unique_targets]\n",
    "\n",
    "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
    "fig = ff.create_annotated_heatmap(cm, x, y)\n",
    "\n",
    "# Set titles and ordering\n",
    "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
    "                    yaxis = dict(categoryorder = \"category descending\")\n",
    "                    )\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=0.5,\n",
    "                        y=-0.15,\n",
    "                        showarrow=False,\n",
    "                        text=\"Predicted label\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=-0.15,\n",
    "                        y=0.5,\n",
    "                        showarrow=False,\n",
    "                        text=\"Actual label\",\n",
    "                        textangle=-90,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# We need margins so the titles fit\n",
    "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
    "fig['data'][0]['showscale'] = True\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Load and print umbiased set\n",
    "#Import the data from the .csv file\n",
    "balanced_dataset = pandas.read_csv('snow_objects_balanced.csv', delimiter=\"\\t\")\n",
    "\n",
    "#Let's have a look at the data\n",
    "graphing.multiple_histogram(balanced_dataset, label_x=\"label\", label_group=\"label\", title=\"Label distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new label with true/false values to our dataset\n",
    "balanced_dataset[\"is_hiker\"] = balanced_dataset.label == \"hiker\"\n",
    "\n",
    "hikers_dataset = balanced_dataset[balanced_dataset[\"is_hiker\"] == 1] \n",
    "nonhikers_dataset = balanced_dataset[balanced_dataset[\"is_hiker\"] == False] \n",
    "# take a random sampling of non-hikers the same size as the hikers subset\n",
    "nonhikers_dataset = nonhikers_dataset.sample(n=len(hikers_dataset.index), random_state=1)\n",
    "balanced_dataset = pandas.concat([hikers_dataset, nonhikers_dataset])\n",
    "\n",
    "# Plot frequency for \"is_hiker\" labels\n",
    "graphing.multiple_histogram(balanced_dataset, label_x=\"is_hiker\", label_group=\"is_hiker\", title=\"Label distribution in balanced dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Test the model using a balanced dataset\n",
    "actual = balanced_dataset.is_hiker\n",
    "predictions = model.predict(balanced_dataset[features])\n",
    "\n",
    "# Build and print our confusion matrix, using the actual values and predictions \n",
    "# from the test set, calculated in previous cells\n",
    "cm = confusion_matrix(actual, predictions, normalize=None)\n",
    "\n",
    "# Print accuracy using this set\n",
    "print(\"Balanced set accuracy:\", assess_accuracy(model,balanced_dataset, \"is_hiker\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot new confusion matrix\n",
    "# Create the list of unique labels in the test set to use in our plot\n",
    "unique_targets = sorted(list(balanced_dataset[\"is_hiker\"].unique()))\n",
    "\n",
    "# Convert values to lower case so the plot code can count the outcomes\n",
    "x = y = [str(s).lower() for s in unique_targets]\n",
    "\n",
    "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
    "fig = ff.create_annotated_heatmap(cm, x, y)\n",
    "\n",
    "# Set titles and ordering\n",
    "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
    "                    yaxis = dict(categoryorder = \"category descending\")\n",
    "                    )\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=0.5,\n",
    "                        y=-0.15,\n",
    "                        showarrow=False,\n",
    "                        text=\"Predicted label\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=-0.15,\n",
    "                        y=0.5,\n",
    "                        showarrow=False,\n",
    "                        text=\"Actual label\",\n",
    "                        textangle=-90,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# We need margins so the titles fit\n",
    "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
    "fig['data'][0]['showscale'] = True\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import function used in calculating weights\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Retrain model using class weights\n",
    "# Using class_weight=\"balanced\" tells the algorithm to automatically calculate weights for us\n",
    "weighted_model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False, class_weight=\"balanced\")\n",
    "# Train the weighted_model using binary label\n",
    "weighted_model.fit(train[features], train.is_hiker)\n",
    "\n",
    "print(\"Train accuracy:\", assess_accuracy(weighted_model,train, \"is_hiker\"))\n",
    "print(\"Test accuracy:\", assess_accuracy(weighted_model, test, \"is_hiker\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Balanced set accuracy:\", assess_accuracy(weighted_model, balanced_dataset, \"is_hiker\"))\n",
    "\n",
    "# Test the weighted_model using a balanced dataset\n",
    "actual = balanced_dataset.is_hiker\n",
    "predictions = weighted_model.predict(balanced_dataset[features])\n",
    "\n",
    "# Build and print our confusion matrix, using the actual values and predictions \n",
    "# from the test set, calculated in previous cells\n",
    "cm = confusion_matrix(actual, predictions, normalize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
    "fig = ff.create_annotated_heatmap(cm, x, y)\n",
    "\n",
    "# Set titles and ordering\n",
    "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
    "                    yaxis = dict(categoryorder = \"category descending\")\n",
    "                    )\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=0.5,\n",
    "                        y=-0.15,\n",
    "                        showarrow=False,\n",
    "                        text=\"Predicted label\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=-0.15,\n",
    "                        y=0.5,\n",
    "                        showarrow=False,\n",
    "                        text=\"Actual label\",\n",
    "                        textangle=-90,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# We need margins so the titles fit\n",
    "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
    "fig['data'][0]['showscale'] = True\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xercise: More metrics derived from confusion matrices\n",
    "In this exercise we will learn about different metrics, using them to explain the results obtained from the binary classification model we built in the previous unit.\n",
    "\n",
    "Data visualization\n",
    "We will use the dataset with different classes of objects found on the mountain one more time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\n",
    "\n",
    "#Import the data from the .csv file\n",
    "dataset = pandas.read_csv('snow_objects.csv', delimiter=\"\\t\")\n",
    "\n",
    "#Let's have a look at the data\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Add a new label with true/false values to our dataset\n",
    "dataset[\"is_hiker\"] = dataset.label == \"hiker\"\n",
    "\n",
    "# Split the dataset in an 70/30 train/test ratio. \n",
    "train, test = train_test_split(dataset, test_size=0.3, random_state=1, shuffle=True)\n",
    "\n",
    "# define a random forest model\n",
    "model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False)\n",
    "\n",
    "# Define which features are to be used \n",
    "features = [\"size\", \"roughness\", \"motion\"]\n",
    "\n",
    "# Train the model using the binary label\n",
    "model.fit(train[features], train.is_hiker)\n",
    "\n",
    "print(\"Model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn has a very convenient utility to build confusion matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "# Calculate the model's accuracy on the TEST set\n",
    "actual = test.is_hiker\n",
    "predictions = model.predict(test[features])\n",
    "\n",
    "# Build and print our confusion matrix, using the actual values and predictions \n",
    "# from the test set, calculated in previous cells\n",
    "cm = confusion_matrix(actual, predictions, normalize=None)\n",
    "\n",
    "# Create the list of unique labels in the test set, to use in our plot\n",
    "# I.e., ['True', 'False',]\n",
    "unique_targets = sorted(list(test[\"is_hiker\"].unique()))\n",
    "\n",
    "# Convert values to lower case so the plot code can count the outcomes\n",
    "x = y = [str(s).lower() for s in unique_targets]\n",
    "\n",
    "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
    "fig = ff.create_annotated_heatmap(cm, x, y)\n",
    "\n",
    "# Set titles and ordering\n",
    "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
    "                    yaxis = dict(categoryorder = \"category descending\"))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=0.5,\n",
    "                        y=-0.15,\n",
    "                        showarrow=False,\n",
    "                        text=\"Predicted label\",\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
    "                        x=-0.15,\n",
    "                        y=0.5,\n",
    "                        showarrow=False,\n",
    "                        text=\"Actual label\",\n",
    "                        textangle=-90,\n",
    "                        xref=\"paper\",\n",
    "                        yref=\"paper\"))\n",
    "\n",
    "# We need margins so the titles fit\n",
    "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
    "fig['data'][0]['showscale'] = True\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also calculate some values that will be used throughout this exercise\n",
    "# We already have actual values and corresponding predictions, defined above\n",
    "correct = actual == predictions\n",
    "tp = numpy.sum(correct & actual)\n",
    "tn = numpy.sum(correct & numpy.logical_not(actual))\n",
    "fp = numpy.sum(numpy.logical_not(correct) & actual)\n",
    "fn = numpy.sum(numpy.logical_not(correct) & numpy.logical_not(actual))\n",
    "\n",
    "print(\"TP - True Positives: \", tp)\n",
    "print(\"TN - True Negatives: \", tn)\n",
    "print(\"FP - False positives: \", fp)\n",
    "print(\"FN - False negatives: \", fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "# len(actual) is the number of samples in the set that generated TP and TN\n",
    "accuracy = (tp+tn) / len(actual) \n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model accuracy is {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for sensitivity/recall\n",
    "sensitivity = recall = tp / (tp + fn)\n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model sensitivity/recall is {sensitivity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for specificity\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model specificity is {specificity:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for precision\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model precision is {precision:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for false positive rate\n",
    "false_positive_rate = fp / (fp + tn)\n",
    "\n",
    "# print result as a percentage\n",
    "print(f\"Model false positive rate is {false_positive_rate:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Good and Bad ROC curves\n",
    "In this exercise, we will make some ROC curves to explain what good and bad ROC curves might look like.\n",
    "\n",
    "The goal of our models is to identify whether each item detected on the mountain is a hiker (true) or a tree (false). Let's take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "!pip install statsmodels\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/hiker_or_tree.csv\n",
    "import graphing # custom graphing code. See our GitHub repo for details\n",
    "import sklearn.model_selection\n",
    "\n",
    "# Load our data from disk\n",
    "df = pandas.read_csv(\"hiker_or_tree.csv\", delimiter=\"\\t\")\n",
    "\n",
    "# Split into train and test\n",
    "train, test =  sklearn.model_selection.train_test_split(df, test_size=0.5, random_state=1)\n",
    "\n",
    "# Graph our three features\n",
    "graphing.histogram(test, label_x=\"height\", label_colour=\"is_hiker\", show=True)\n",
    "graphing.multiple_histogram(test, label_x=\"motion\", label_group=\"is_hiker\", nbins=12, show=True)\n",
    "graphing.multiple_histogram(test, label_x=\"texture\", label_group=\"is_hiker\", nbins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api\n",
    "\n",
    "# Create a fake model that is perfect at predicting labels\n",
    "class PerfectModel:\n",
    "    def predict(self, x):\n",
    "        # The perfect model believes that hikers are all\n",
    "        # under 4m tall\n",
    "        return 1 / (1 + numpy.exp(80*(x - 4)))\n",
    "    \n",
    "model = PerfectModel()\n",
    "\n",
    "# Plot the model\n",
    "import graphing\n",
    "graphing.scatter_2D(test, trendline=model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tpr_fpr(prediction, actual):\n",
    "    '''\n",
    "    Calculates true positive rate and false positive rate\n",
    "\n",
    "    prediction: the labels predicted by the model\n",
    "    actual:     the correct labels we hope the model predicts\n",
    "    '''\n",
    "\n",
    "    # To calculate the true positive rate and true negative rate we need to know\n",
    "    # TP - how many true positives (where the model predicts hiker, and it is a hiker)\n",
    "    # TN - how many true negatives (where the model predicts tree, and it is a tree)\n",
    "    # FP - how many false positives (where the model predicts hiker, but it was a tree)\n",
    "    # FN - how many false negatives (where the model predicts tree, but it was a hiker)\n",
    "\n",
    "    # First, make a note of which predictions were 'true' and which were 'false'\n",
    "    prediction_true = numpy.equal(prediction, 1)\n",
    "    prediction_false= numpy.equal(prediction, 0)\n",
    "\n",
    "    # Now, make a note of which correct results were 'true' and which were 'false'\n",
    "    actual_true = numpy.equal(actual, 1)\n",
    "    actual_false = numpy.equal(actual, 0)\n",
    "\n",
    "    # Calculate TP, TN, FP, and FN\n",
    "    # The combination of sum and '&' counts the overlap\n",
    "    # For example, TP calculates how many 'true' predictions \n",
    "    # overlapped with 'true' labels (correct answers)\n",
    "    TP = numpy.sum(prediction_true  & actual_true)\n",
    "    TN = numpy.sum(prediction_false & actual_false)\n",
    "    FP = numpy.sum(prediction_true  & actual_false)\n",
    "    FN = numpy.sum(prediction_false & actual_true)\n",
    "\n",
    "    # Calculate the true positive rate\n",
    "    # This is the proportion of 'hiker' labels that are identified as hikers\n",
    "    tpr = TP / (TP + FN)\n",
    "\n",
    "    # Calculate the false positive rate \n",
    "    # This is the proportion of 'tree' labels that are identified as hikers\n",
    "    fpr = FP / (FP + TN)\n",
    "\n",
    "    # Return both rates\n",
    "    return tpr, fpr\n",
    "\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model(model_predict, feature_name, threshold):\n",
    "    '''\n",
    "    Calculates the true positive rate and false positive rate of the model\n",
    "    at a particular decision threshold\n",
    "\n",
    "    model_predict: the model's predict function\n",
    "    feature_name: the feature the model is expecting\n",
    "    threshold: the decision threshold to use \n",
    "    '''\n",
    "\n",
    "    # Make model predictions for every sample in the test set\n",
    "    # What we get back is a probability that the sample is a hiker\n",
    "    # For example, if we had two samples in the test set, we might\n",
    "    # get 0.45 and 0.65, meaning the model says there is a 45% chance\n",
    "    # the first sample is a hiker, and 65% chance the second is a \n",
    "    # hiker\n",
    "    probability_of_hiker = model_predict(test[feature_name])\n",
    "    \n",
    "    # See which predictions at this threshold would say hiker\n",
    "    predicted_is_hiker = probability_of_hiker > threshold\n",
    "\n",
    "    # calculate the true and false positives rates using our\n",
    "    # handy method\n",
    "    return calculate_tpr_fpr(predicted_is_hiker, test.is_hiker)\n",
    "\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef create_roc_curve(model_predict, feature=\"height\"):\n",
    "    '''\n",
    "    This function creates a ROC curve for a given model by testing it\n",
    "    on the test set for a range of decision thresholds. An ROC curve has\n",
    "    the True Positive rate on the x-axis and False Positive rate on the \n",
    "    y-axis\n",
    "\n",
    "    model_predict: The model's predict function\n",
    "    feature: The feature to provide the model's predict function\n",
    "    '''\n",
    "\n",
    "    # Calculate what the true positive and false positive rate would be if\n",
    "    # we had used different thresholds. \n",
    "\n",
    "    #  Make a list of thresholds to try\n",
    "    thresholds = numpy.linspace(0,1,101)\n",
    "\n",
    "    false_positive_rates = []\n",
    "    true_positive_rates = []\n",
    "\n",
    "    # Loop through all thresholds\n",
    "    for threshold in thresholds:\n",
    "        # calculate the true and false positives rates using our\n",
    "        # handy method\n",
    "        tpr, fpr = assess_model(model_predict, feature, threshold)\n",
    "\n",
    "        # save the results\n",
    "        true_positive_rates.append(tpr)\n",
    "        false_positive_rates.append(fpr)\n",
    "\n",
    "\n",
    "    # Graph the result\n",
    "    # You don't need to understand this code, but essentially we are plotting\n",
    "    # TPR versus FPR as a line plot\n",
    "    # -- Prepare a dataframe, required by our graphing code\n",
    "    df_for_graphing = pandas.DataFrame(dict(fpr=false_positive_rates, tpr=true_positive_rates, threshold=thresholds))\n",
    "    # -- Generate the plot\n",
    "    fig = graphing.scatter_2D(df_for_graphing, x_range=[-0.05,1.05])\n",
    "    fig.update_traces(mode='lines') # Comment our this line if you would like to see points rather than lines\n",
    "    fig.update_yaxes(range=[-0.05, 1.05])\n",
    "\n",
    "    # Display the graph\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Create an roc curve for our model\n",
    "create_roc_curve(model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake model that gets every single answer incorrect\n",
    "class VeryBadModel:\n",
    "    def predict(self, x):\n",
    "        # This model thinks that all people are over 4m tall \n",
    "        # and all trees are shorter\n",
    "        return 1 / (1 + numpy.exp(-80*(x - 4)))\n",
    "\n",
    "model = VeryBadModel()\n",
    "\n",
    "# Plot the model\n",
    "graphing.scatter_2D(test, trendline=model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our code to create the ROC curve\n",
    "create_roc_curve(model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport statsmodels.api\n",
    "\n",
    "# This is a helper method that reformats the data to be compatible\n",
    "# with this particular logistic regression model \n",
    "prep_data = lambda x:  numpy.column_stack((numpy.full(x.shape, 1), x))\n",
    "\n",
    "# Train a logistic regression model to predict hiker based on texture\n",
    "model = statsmodels.api.Logit(train.is_hiker, prep_data(train.texture)).fit()\n",
    "\n",
    "# Plot the model\n",
    "graphing.scatter_2D(test, label_x=\"texture\", label_y=\"is_hiker\", trendline=lambda x: model.predict(prep_data(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our code to create the ROC curve\n",
    "create_roc_curve(lambda x: model.predict(prep_data(x)), \"texture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api\n",
    "\n",
    "# Train a logistic regression model to predict hiker based on motion\n",
    "model = statsmodels.api.Logit(train.is_hiker, prep_data(train.motion), add_constant=True).fit()\n",
    "\n",
    "# Plot the model\n",
    "graphing.scatter_2D(test, label_x=\"motion\", label_y=\"is_hiker\", trendline=lambda x: model.predict(prep_data(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_roc_curve(lambda x: model.predict(prep_data(x)), \"motion\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Tune the area under the curve\n",
    "In this exercise, we will make and compare two models, using ROC curves, and tune one using the area under the curve (AUC).\n",
    "\n",
    "The goal of our models is to identify whether each item detected on the mountain is a hiker (true) or a tree (false). We will work with our motion feature here. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "!pip install statsmodels\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/hiker_or_tree.csv\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/m2d_make_roc.py\n",
    "import graphing # custom graphing code. See our GitHub repo for details\n",
    "import sklearn.model_selection\n",
    "\n",
    "# Load our data from disk\n",
    "df = pandas.read_csv(\"hiker_or_tree.csv\", delimiter=\"\\\\t\")\n",
    "\n",
    "# Remove features we no longer want\n",
    "del df[\"height\"]\n",
    "del df[\"texture\"]\n",
    "\n",
    "# Split into train and test\n",
    "train, test =  sklearn.model_selection.train_test_split(df, test_size=0.5, random_state=1)\n",
    "\n",
    "# Graph our feature\n",
    "graphing.multiple_histogram(test, label_x=\"motion\", label_group=\"is_hiker\", nbins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport statsmodels.api\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# This is a helper method that reformats the data to be compatible\n",
    "# with this particular logistic regression model \n",
    "prep_data = lambda x:  numpy.column_stack((numpy.full(x.shape, 1), x))\n",
    "\n",
    "# Train a logistic regression model to predict hiker based on motion\n",
    "lr_model = statsmodels.api.Logit(train.is_hiker, prep_data(train.motion), add_constant=True).fit()\n",
    "\n",
    "# Assess its performance\n",
    "# -- Train\n",
    "predictions = lr_model.predict(prep_data(train.motion)) > 0.5\n",
    "train_accuracy = accuracy_score(train.is_hiker, predictions)\n",
    "\n",
    "# -- Test\n",
    "predictions = lr_model.predict(prep_data(test.motion)) > 0.5\n",
    "test_accuracy = accuracy_score(test.is_hiker, predictions)\n",
    "\n",
    "print(\"Train accuracy\", train_accuracy)\n",
    "print(\"Test accuracy\", test_accuracy)\n",
    "\n",
    "# Plot the model\n",
    "predict_with_logistic_regression = lambda x: lr_model.predict(prep_data(x))\n",
    "graphing.scatter_2D(test, label_x=\"motion\", label_y=\"is_hiker\", title=\"Logistic Regression\", trendline=predict_with_logistic_regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a random forest model with 50 trees\n",
    "random_forest = RandomForestClassifier(random_state=2,\n",
    "                                       verbose=False)\n",
    "\n",
    "# Train the model\n",
    "random_forest.fit(train[[\"motion\"]], train.is_hiker)\n",
    "\n",
    "# Assess its performance\n",
    "# -- Train\n",
    "predictions = random_forest.predict(train[[\"motion\"]])\n",
    "train_accuracy = accuracy_score(train.is_hiker, predictions)\n",
    "\n",
    "# -- Test\n",
    "predictions = random_forest.predict(test[[\"motion\"]])\n",
    "test_accuracy = accuracy_score(test.is_hiker, predictions)\n",
    "\n",
    "\n",
    "# Train and test the model\n",
    "print(\"Random Forest Performance:\")\n",
    "print(\"Train accuracy\", train_accuracy)\n",
    "print(\"Test accuracy\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from m2d_make_roc import create_roc_curve # import our previous ROC code\n",
    "\n",
    "fig, thresholds_lr = create_roc_curve(predict_with_logistic_regression, test, \"motion\")\n",
    "\n",
    "# Uncomment the line below if you would like to see the area under the curve\n",
    "#fig.update_traces(fill=\"tozeroy\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Show the table of results\n",
    "thresholds_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about this lambda function. It simply reorganizes \n",
    "# the data into the shape expected by the random forest model, \n",
    "# and calls predict_proba, which gives us predicted probabilities\n",
    "# that the label is 'hiker'\n",
    "predict_with_random_forest = lambda x: random_forest.predict_proba(numpy.array(x).reshape(-1, 1))[:,1]\n",
    "\n",
    "# Create the ROC curve\n",
    "fig, thresholds_rf = create_roc_curve(predict_with_random_forest, test, \"motion\")\n",
    "\n",
    "# Uncomment the line below if you would like to see the area under the curve\n",
    "#fig.update_traces(fill=\"tozeroy\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Show the table of results\n",
    "thresholds_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Logistic regression\n",
    "print(\"Logistic Regression AUC:\", roc_auc_score(test.is_hiker, predict_with_logistic_regression(test.motion)))\n",
    "\n",
    "# Random Forest\n",
    "print(\"Random Forest AUC:\", roc_auc_score(test.is_hiker, predict_with_random_forest(test.motion)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out its expected performance at the default threshold of 0.5\n",
    "# We previously obtained this information when we created our graphs\n",
    "row_of_0point5 = thresholds_rf[thresholds_rf.threshold == 0.5]\n",
    "print(\"TPR at threshold of 0.5:\", row_of_0point5.tpr.values[0])\n",
    "print(\"FPR at threshold of 0.5:\", row_of_0point5.fpr.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how good each threshold is from our TPR and FPR. \n",
    "# Our criteria is that the TPR is as high as possible and \n",
    "# the FPR is as low as possible. We consider them equally important\n",
    "scores = thresholds_rf.tpr - thresholds_rf.fpr\n",
    "\n",
    "# Find the entry with the lowest score according to our criteria\n",
    "index_of_best_score = numpy.argmax(scores)\n",
    "best_threshold = thresholds_rf.threshold[index_of_best_score]\n",
    "print(\"Best threshold:\", best_threshold)\n",
    "\n",
    "# Print out its expected performance\n",
    "print(\"TPR at this threshold:\", thresholds_rf.tpr[index_of_best_score])\n",
    "print(\"FPR at this threshold:\", thresholds_rf.fpr[index_of_best_score])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
